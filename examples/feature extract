from __future__ import print_function, absolute_import
import argparse
import os.path as osp
import scipy.io as sio
import numpy as np
import sys
import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader
from reid.datasets import get_dataset   
from reid.models import ResNet
from reid.evaluators import extract_features
from reid.utils.data import transforms
from reid.utils.data.preprocessor import Preprocessor
from reid.utils.logging import Logger
from reid.utils.serialization import load_checkpoint



def main(args):
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)

    cudnn.benchmark = True

    # Redirect print to both console and log file
    if not args.evaluate:
        sys.stdout = Logger(osp.join(args.logs_dir, 'log.txt'))


    # Redirect print to both console and log file
    if not args.evaluate:
        sys.stdout = Logger(osp.join(args.logs_dir, 'log.txt'))

    if args.loss == 'triplet':
        assert args.num_instances > 1, 'TripletLoss requires num_instances > 1'
        assert args.batch_size % args.num_instances == 0, \
            'num_instances should divide batch_size'


    if args.loss == 'xentropy':
        model = ResNet(args.depth, pretrained=True,
                       num_classes=4,
                       num_features=args.features, dropout=args.dropout)
    elif args.loss == 'oim':
        model = ResNet(args.depth, pretrained=True, num_features=args.features,
                       norm=True, dropout=args.dropout)
    elif args.loss == 'triplet':
        # Hack for making the classifier the last feature embedding layer
        # Net structure: avgpool -> FC(1024) -> FC(args.features)
        model = ResNet(args.depth, pretrained=True,
                       num_features=1024,
                       norm=False, dropout=args.dropout,
                       num_classes=args.features)
    else:
        raise ValueError("Cannot recognize loss type:", args.loss)
    model = torch.nn.DataParallel(model).cuda()

    # Load from checkpoint        这一块是干嘛的？
    if args.resume:
        checkpoint = load_checkpoint(args.resume)
        model.load_state_dict(checkpoint['state_dict'])
        args.start_epoch = checkpoint['epoch']
        best_top1 = checkpoint['best_top1']
        print("=> start epoch {}  best top1 {:.1%}"
              .format(args.start_epoch, best_top1))
    else:
        best_top1 = 0

    normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                      std=[0.229, 0.224, 0.225])
    test_sets = get_dataset('new2', '/home/lin/re_id/open-reid-master_1/examples/test_data')
    test_set = test_sets.wwwww
    test_loader = DataLoader(
        Preprocessor(test_set,
                     root=test_sets.images_dir,
                     transform=transforms.Compose([
                         transforms.RectScale(256, 128),
                         transforms.ToTensor(),
                         normalizer,
                     ])),
        batch_size=1, num_workers=2,
        shuffle=False, pin_memory=True)
    features, labels = extract_features(model, test_loader)
    sio.savemat('/home/lin/re_id/open-reid-master_1/examples/test_data/train.mat', features)
    
    
    
    if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ID Training ResNet Model")
    # data  
    # parser.add_argument('-d', '--dataset', type=str, default='cuhk03',
    #                     choices=['cuhk03', 'market1501', 'viper', 'dukemtmc',])
    parser.add_argument('-d', '--dataset', type=str, default='new2')
    parser.add_argument('-b', '--batch-size', type=int, default=8)
    parser.add_argument('-j', '--workers', type=int, default=0)
    parser.add_argument('--split', type=int, default=0)
    parser.add_argument('--num-instances', type=int, default=2,
                        help="If greater than zero, each minibatch will"
                             "consist of (batch_size // num_instances)"
                             "identities, and each identity will have"
                             "num_instances instances. Used in conjunction with"
                             "--loss triplet")
    parser.add_argument('--combine-trainval', action='store_true',
                        help="Use train and val sets together for training."
                             "Val set is still used for validation.")
    # model  
    parser.add_argument('--depth', type=int, default=50,                     
                        choices=[18, 34, 50, 101, 152])
    parser.add_argument('--features', type=int, default=128)
    parser.add_argument('--dropout', type=float, default=0.5)
    # loss     
    parser.add_argument('--loss', type=str, default='triplet',
                        choices=['xentropy', 'oim', 'triplet'])
    parser.add_argument('--oim-scalar', type=float, default=30)          
    parser.add_argument('--oim-momentum', type=float, default=0.5)
    parser.add_argument('--triplet-margin', type=float, default=0.5)
    # optimizer   
    parser.add_argument('--optimizer', type=str, default='sgd',
                        choices=['sgd', 'adam'])
    parser.add_argument('--lr', type=float, default=0.01)
    parser.add_argument('--momentum', type=float, default=0.9)
    parser.add_argument('--weight-decay', type=float, default=5e-4)
    # training configs   
    parser.add_argument('--resume', type=str, default='/home/lin/resnet-new-triplet-100000/checkpoint.pth.tar', metavar='PATH') #这一行是什么意思？
    #parser.add_argument('--resume', type=str, default='/home/lin/re_id/open-reid-master_1/examples/logs/checkpoint.pth.tar', metavar='PATH')
    parser.add_argument('--evaluate', action='store_true')
    parser.add_argument('--start-epoch', type=int, default=0)
    parser.add_argument('--epochs', type=int, default=1)
    parser.add_argument('--seed', type=int, default=1)
    parser.add_argument('--print-freq', type=int, default=1)
    # metric learning    
    parser.add_argument('--dist-metric', type=str, default='euclidean',
                        choices=['euclidean', 'kissme'])
    # misc  
    working_dir = osp.dirname(osp.abspath(__file__))
    parser.add_argument('--data-dir', type=str, metavar='PATH',
                        default=osp.join(working_dir, 'data'))      
    parser.add_argument('--logs-dir', type=str, metavar='PATH',
                        default=osp.join(working_dir, 'logs'))
    main(parser.parse_args())


